{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a690cea9-78f3-4650-8e32-4549dd61894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "\n",
    "R-squared, also known as the coefficient of determination, is a statistical measure used to assess the goodness of fit of a regression model. It provides a measure of how well the independent variable(s) explain the variability of the dependent variable in a regression model.\n",
    "\n",
    "R-squared is calculated by comparing the variance of the dependent variable explained by the regression model to the total variance of the dependent variable. It is defined as the proportion of the total sum of squares (SSR) explained by the regression model compared to the total sum of squares (SST). Mathematically, it can be expressed as:\n",
    "\n",
    "�\n",
    "2\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "R \n",
    "2\n",
    " = \n",
    "SST\n",
    "SSR\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "�\n",
    "�\n",
    "SSR is the sum of squared differences between the predicted values and the mean of the dependent variable.\n",
    "�\n",
    "�\n",
    "�\n",
    "SST is the total sum of squares, which is the sum of squared differences between each observed value of the dependent variable and the mean of the dependent variable.\n",
    "R-squared ranges from 0 to 1, where:\n",
    "\n",
    "�\n",
    "2\n",
    "=\n",
    "0\n",
    "R \n",
    "2\n",
    " =0 indicates that the regression model does not explain any of the variability of the dependent variable around its mean.\n",
    "�\n",
    "2\n",
    "=\n",
    "1\n",
    "R \n",
    "2\n",
    " =1 indicates that the regression model perfectly explains all of the variability of the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafba53b-979b-4ce6-9bd4-f0665dcb4947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "\n",
    "\n",
    "Adjusted R-squared is a modified version of the regular R-squared that takes into account the number of predictors in a regression model. While regular R-squared increases whenever a new predictor is added to the model, adjusted R-squared penalizes the addition of unnecessary predictors that do not significantly improve the model's explanatory power.\n",
    "\n",
    "Adjusted R-squared is calculated using the formula:\n",
    "\n",
    "Adjusted \n",
    "�\n",
    "2\n",
    "=\n",
    "1\n",
    "−\n",
    "(\n",
    "1\n",
    "−\n",
    "�\n",
    "2\n",
    ")\n",
    "(\n",
    "�\n",
    "−\n",
    "1\n",
    ")\n",
    "�\n",
    "−\n",
    "�\n",
    "−\n",
    "1\n",
    "Adjusted R \n",
    "2\n",
    " =1− \n",
    "n−k−1\n",
    "(1−R \n",
    "2\n",
    " )(n−1)\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "2\n",
    "R \n",
    "2\n",
    "  is the regular R-squared.\n",
    "�\n",
    "n is the number of observations in the sample.\n",
    "�\n",
    "k is the number of independent variables (predictors) in the regression model.\n",
    "Adjusted R-squared adjusts \n",
    "�\n",
    "2\n",
    "R \n",
    "2\n",
    "  downwards as the number of predictors increases, reflecting the degree of complexity added by including more predictors. This adjustment helps prevent overfitting, where a model captures noise or random fluctuations in the data rather than underlying relationships. By penalizing the inclusion of unnecessary predictors, adjusted R-squared provides a more conservative estimate of the model's explanatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c3a8a-7e9a-484b-86c8-5af4deec7e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "\n",
    "Adjusted R-squared is more appropriate to use in situations where you want to compare the goodness of fit of regression models with different numbers of predictors or when you want to guard against overfitting.\n",
    "\n",
    "Here are some situations where adjusted R-squared is particularly useful:\n",
    "\n",
    "Comparing Models with Different Numbers of Predictors: Adjusted R-squared adjusts for the number of predictors in the model, allowing for a fair comparison of models with varying complexities. This is especially important when comparing nested models or models with different numbers of independent variables.\n",
    "\n",
    "Preventing Overfitting: Overfitting occurs when a model captures noise or random fluctuations in the data rather than underlying relationships. Adjusted R-squared penalizes the inclusion of unnecessary predictors, helping to guard against overfitting by providing a more conservative estimate of the model's explanatory power.\n",
    "\n",
    "Model Selection: When selecting the best model from a set of candidates, adjusted R-squared can help identify the model that strikes the right balance between goodness of fit and simplicity. Models with higher adjusted R-squared values and fewer predictors are generally preferred, as they are more likely to generalize well to new data.\n",
    "\n",
    "Interpreting Model Performance: Adjusted R-squared provides a more accurate assessment of a model's goodness of fit, taking into account both the explanatory power and the complexity of the model. It offers a clearer understanding of how well the model fits the data and how much of the variability in the dependent variable is explained by the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55103923-0729-4c15-b450-e50c171d2302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4\n",
    "\n",
    "RMSE, MSE, and MAE are common metrics used to evaluate the performance of regression models. They quantify the difference between the predicted values and the actual values of the dependent variable.\n",
    "\n",
    "Root Mean Squared Error (RMSE):\n",
    "\n",
    "RMSE is a measure of the average deviation between the predicted and actual values of the dependent variable.\n",
    "It is calculated by taking the square root of the average of the squared differences between the predicted and actual values.\n",
    "Mathematically, RMSE is calculated as:\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "^\n",
    "�\n",
    ")\n",
    "2\n",
    "RMSE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "​\n",
    " \n",
    "RMSE is sensitive to outliers and penalizes large errors more heavily due to the squaring operation.\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "MSE is similar to RMSE but without taking the square root. It measures the average of the squared differences between the predicted and actual values.\n",
    "It is calculated as:\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "^\n",
    "�\n",
    ")\n",
    "2\n",
    "MSE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "Like RMSE, MSE penalizes larger errors more heavily due to squaring.\n",
    "Mean Absolute Error (MAE):\n",
    "\n",
    "MAE is a measure of the average absolute difference between the predicted and actual values of the dependent variable.\n",
    "It is calculated by taking the average of the absolute differences between the predicted and actual values.\n",
    "Mathematically, MAE is calculated as:\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∣\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "^\n",
    "�\n",
    "∣\n",
    "MAE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " ∣y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ∣\n",
    "Unlike RMSE and MSE, MAE does not square the errors, making it less sensitive to outliers.\n",
    "Interpretation:\n",
    "\n",
    "RMSE, MSE, and MAE provide a measure of the accuracy of the regression model's predictions.\n",
    "Lower values of RMSE, MSE, and MAE indicate better model performance, with the ideal value being zero (perfect prediction).\n",
    "RMSE and MSE are commonly used when larger errors should be penalized more heavily, such as in cases where small errors are acceptable but large errors are problematic.\n",
    "MAE is preferred when outliers are present and need to be handled more robustly, as it gives equal weight to all errors regardless of their magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630dab48-903b-42b4-841e-b244c6b30611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5\n",
    "\n",
    "Advantages:\n",
    "\n",
    "RMSE (Root Mean Squared Error):\n",
    "\n",
    "Advantage: RMSE penalizes large errors more heavily due to the squaring operation, making it sensitive to outliers. This is useful in cases where large errors are particularly undesirable.\n",
    "Advantage: RMSE is widely used and easily interpretable as it represents the standard deviation of the residuals.\n",
    "Advantage: RMSE is a differentiable function, making it suitable for optimization algorithms.\n",
    "MSE (Mean Squared Error):\n",
    "\n",
    "Advantage: MSE, like RMSE, penalizes large errors more heavily due to the squaring operation, providing a measure of the average squared deviation between the predicted and actual values.\n",
    "Advantage: MSE is also widely used and is mathematically convenient for optimization algorithms.\n",
    "MAE (Mean Absolute Error):\n",
    "\n",
    "Advantage: MAE is less sensitive to outliers compared to RMSE and MSE because it does not square the errors. This makes MAE more robust in the presence of outliers.\n",
    "Advantage: MAE provides a straightforward interpretation as it represents the average absolute deviation between the predicted and actual values.\n",
    "Advantage: MAE is also less affected by large errors and can be more suitable for situations where the absolute magnitude of errors is more important than their squared values.\n",
    "Disadvantages:\n",
    "\n",
    "RMSE (Root Mean Squared Error):\n",
    "\n",
    "Disadvantage: RMSE's sensitivity to outliers can be a disadvantage in some cases, especially when outliers are not necessarily indicative of poor model performance.\n",
    "Disadvantage: RMSE is heavily influenced by large errors due to the squaring operation, which may not always reflect the true performance of the model.\n",
    "MSE (Mean Squared Error):\n",
    "\n",
    "Disadvantage: Similar to RMSE, MSE's sensitivity to outliers can be a disadvantage in situations where outliers are not necessarily indicative of poor model performance.\n",
    "Disadvantage: Like RMSE, MSE is heavily influenced by large errors due to the squaring operation, which may not always reflect the true performance of the model.\n",
    "MAE (Mean Absolute Error):\n",
    "\n",
    "Disadvantage: MAE may not penalize large errors enough compared to RMSE and MSE, which could lead to underestimation of the impact of outliers on model performance.\n",
    "Disadvantage: MAE does not have the mathematical convenience of being differentiable, which can be a limitation when using optimization algorithms that rely on gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff9f0c-d544-4831-bd40-9365fa542668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6\n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression and other regression models to prevent overfitting and improve the model's performance by penalizing the magnitude of the coefficients. It adds a penalty term to the loss function that encourages the coefficients of less important features to be exactly zero, effectively performing feature selection by eliminating some of the predictors from the model.\n",
    "\n",
    "The Lasso regularization term is represented as:\n",
    "\n",
    "Lasso regularization term\n",
    "=\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    "Lasso regularization term=λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣\n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "λ is the regularization parameter, which controls the strength of the regularization. Higher values of \n",
    "�\n",
    "λ result in more coefficients being pushed towards zero.\n",
    "�\n",
    "p is the number of predictors in the model.\n",
    "�\n",
    "�\n",
    "β \n",
    "j\n",
    "​\n",
    "  are the coefficients of the predictors.\n",
    "The main difference between Lasso and Ridge regularization lies in the penalty term:\n",
    "\n",
    "Lasso uses the \n",
    "�\n",
    "1\n",
    "L \n",
    "1\n",
    "​\n",
    "  norm (absolute values of the coefficients), leading to sparsity in the coefficient vector. This means that some coefficients will be exactly zero, effectively performing feature selection.\n",
    "Ridge regularization, on the other hand, uses the \n",
    "�\n",
    "2\n",
    "L \n",
    "2\n",
    "​\n",
    "  norm (squared values of the coefficients), which does not lead to exact zero coefficients but rather shrinks the coefficients towards zero without eliminating them entirely.\n",
    "When to use Lasso regularization:\n",
    "\n",
    "Feature selection: Lasso is particularly useful when there are a large number of predictors, some of which may be irrelevant or redundant. By setting some coefficients to zero, Lasso effectively performs feature selection, identifying the most important predictors while disregarding the less important ones.\n",
    "Interpretability: When interpretability of the model is important, Lasso can be preferred over Ridge regularization because it results in a sparse model with fewer predictors, making it easier to interpret.\n",
    "Sparse solutions: In cases where a sparse solution is desired, such as when dealing with high-dimensional data or when the true underlying model is believed to be sparse, Lasso regularization is more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3f1b9-08ff-475b-9555-5c4067fea4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 7\n",
    "\n",
    "Regularized linear models, such as Ridge regression and Lasso regression, help prevent overfitting in machine learning by adding a penalty term to the loss function, which penalizes large coefficients. This penalty discourages the model from fitting the training data too closely and instead encourages simpler models that generalize better to unseen data.\n",
    "\n",
    "Here's how regularized linear models prevent overfitting:\n",
    "\n",
    "Penalizing Large Coefficients:\n",
    "\n",
    "Regularization adds a penalty term to the loss function that penalizes large coefficients. This penalty is applied during the model training process.\n",
    "By penalizing large coefficients, regularization discourages the model from fitting the noise or idiosyncrasies in the training data too closely.\n",
    "As a result, the model is less likely to capture random fluctuations in the training data, leading to improved generalization performance on unseen data.\n",
    "Encouraging Simpler Models:\n",
    "\n",
    "Regularization encourages simpler models by shrinking the coefficients towards zero or setting some coefficients exactly to zero.\n",
    "Smaller coefficients lead to smoother and less complex decision boundaries, reducing the model's tendency to overfit the training data.\n",
    "Simpler models are more robust and less likely to capture noise or outliers in the training data, making them better suited for generalization to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551c1fa-91dc-48de-839b-c25637ae806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 8\n",
    "\n",
    "Regularized linear models, such as Ridge regression and Lasso regression, offer significant advantages in mitigating overfitting and improving generalization performance. However, they also come with limitations that may make them less suitable in certain scenarios:\n",
    "\n",
    "Loss of Interpretability:\n",
    "\n",
    "Regularization tends to shrink coefficients towards zero, which can make the model less interpretable, especially in the case of Lasso regression where some coefficients may be exactly zero.\n",
    "In scenarios where interpretability is crucial for understanding the relationship between predictors and the target variable, the loss of interpretability associated with regularization may be undesirable.\n",
    "Inability to Handle Non-Linear Relationships:\n",
    "\n",
    "Regularized linear models assume a linear relationship between predictors and the target variable. However, real-world data often exhibits non-linear relationships that cannot be adequately captured by linear models.\n",
    "In cases where the relationship between predictors and the target variable is non-linear, regularized linear models may not perform well and more flexible non-linear models may be more appropriate.\n",
    "Feature Scaling Sensitivity:\n",
    "\n",
    "Regularized linear models are sensitive to the scale of the features. If features are not properly scaled, features with larger scales may dominate the regularization process, leading to biased coefficient estimates.\n",
    "Ensuring proper feature scaling can be cumbersome, especially in datasets with features of different scales or units.\n",
    "Limited Performance Improvement with Small Datasets:\n",
    "\n",
    "Regularization is particularly effective in preventing overfitting when the dataset is large. In small datasets, the regularization penalty may overly constrain the model, leading to underfitting and reduced performance.\n",
    "In such cases, simpler models without regularization may generalize better to new data.\n",
    "Selection of Regularization Parameter:\n",
    "\n",
    "Regularized linear models require the selection of a regularization parameter (\n",
    "�\n",
    "λ in Ridge and Lasso regression) to control the strength of regularization.\n",
    "Choosing an appropriate regularization parameter can be challenging and may require cross-validation, especially when the dataset size is limited or the relationship between predictors and the target variable is complex.\n",
    "Loss of Information:\n",
    "\n",
    "Regularization, particularly Lasso regression, may set some coefficients exactly to zero, effectively eliminating corresponding predictors from the model.\n",
    "While this can be beneficial for feature selection, it may result in loss of valuable information if important predictors are incorrectly excluded from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afac0fb-0e71-49e1-afd2-055231c735ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 9\n",
    "\n",
    "Choosing between Model A and Model B based solely on their respective RMSE and MAE values depends on the specific requirements and characteristics of the problem at hand. However, let's discuss the implications of each metric and make a decision based on their respective strengths and limitations:\n",
    "\n",
    "Root Mean Squared Error (RMSE):\n",
    "\n",
    "RMSE penalizes large errors more heavily due to the squaring operation, making it sensitive to outliers.\n",
    "In this case, Model A has an RMSE of 10, indicating that, on average, the predicted values deviate from the actual values by approximately 10 units.\n",
    "A lower RMSE value indicates better performance in terms of minimizing overall error magnitude.\n",
    "Mean Absolute Error (MAE):\n",
    "\n",
    "MAE measures the average absolute difference between the predicted and actual values, without squaring the errors.\n",
    "In this case, Model B has an MAE of 8, indicating that, on average, the absolute difference between predicted and actual values is 8 units.\n",
    "MAE provides a straightforward interpretation and is less sensitive to outliers compared to RMSE.\n",
    "Considering the metrics:\n",
    "\n",
    "Model B with an MAE of 8 suggests that, on average, the predictions are off by 8 units, which may be preferable if the goal is to minimize average absolute error without being overly influenced by large errors.\n",
    "However, Model A with an RMSE of 10 may still be preferable if the goal is to minimize overall error magnitude, especially if the dataset contains significant outliers that need to be appropriately penalized.\n",
    "Limitations:\n",
    "\n",
    "Both RMSE and MAE have their own limitations. RMSE is sensitive to outliers due to squaring, while MAE may not penalize large errors enough.\n",
    "Neither metric provides insight into the direction of errors or the distribution of errors across the dataset, which may be important considerations in certain applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e58f8-3eb3-41e2-a272-869642f9d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 10 \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
